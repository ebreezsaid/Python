{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use case: CNN Classification - ImageNet\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        " image classification using Convolutional Neural Networks (CNNs). We'll use the Tiny ImageNet dataset, which contains 8,000 64x64 color images in 10 classes, with 4,000 images per class. There are 50,000 training images and 5,000 test images."
      ],
      "metadata": {
        "id": "kOUwKt1fSF6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download the Tiny ImageNet dataset from Kaggle, you will first need to install the Kaggle API. In a Colab notebook, you can do it as follows:"
      ],
      "metadata": {
        "id": "Bh0NPJiwUEkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE4pm-zKRUJ2",
        "outputId": "3d517870-373e-402a-a622-a2e93b64432d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you need to upload your Kaggle API token. If you don't have one, you can download it from your Kaggle account page (under \"API\")."
      ],
      "metadata": {
        "id": "V74gYCsTUG1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUQT2e8yTrvx",
        "outputId": "e7ed4092-92c3-49e7-dcb5-0448e7ac76ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2465d75b-caca-4e8b-953a-271fdf5de98c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2465d75b-caca-4e8b-953a-271fdf5de98c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can download the Tiny ImageNet dataset. Here is the command you would use to download the dataset:"
      ],
      "metadata": {
        "id": "hG3oHPn0UJYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d chetankv/dogs-cats-images"
      ],
      "metadata": {
        "id": "Zdng83ChUKq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading the dataset, you can unzip it as follows:"
      ],
      "metadata": {
        "id": "Cn1P0wCJUXEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \\*.zip"
      ],
      "metadata": {
        "id": "h0dxXFQAUXeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## oad the Training and Validation Data\n",
        "\n",
        "You can use the image_dataset_from_directory function from tensorflow.keras.preprocessing to load the data:"
      ],
      "metadata": {
        "id": "Ysn5zP_9UlS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories\n",
        "root_dir = '/content/dog vs cat/dataset/'\n",
        "train_dir = '/content/dog vs cat/dataset/training_set'\n",
        "val_dir = '/content/dog vs cat/dataset/test_set'\n"
      ],
      "metadata": {
        "id": "drP1IPjSUrA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_dir, image_size=(64, 64), batch_size=32)\n",
        "val_dataset = image_dataset_from_directory(val_dir, image_size=(64, 64), batch_size=32)"
      ],
      "metadata": {
        "id": "KpAPNmO4WC7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "Performing Exploratory Data Analysis (EDA) on image datasets like Tiny ImageNet involves understanding the distribution of classes, visualizing some samples from each class, and potentially checking the color distribution and size of the images.\n",
        "\n",
        "Performing an Exploratory Data Analysis (EDA) on your dataset provides several benefits, especially when working on a classification problem like cats vs dogs:\n",
        "\n",
        "* Understanding the data distribution: Visualizing the number of samples in each class helps to identify if there's an imbalance in the dataset. In real-world scenarios, datasets often have imbalanced classes, which can impact the performance of machine learning algorithms. They tend to show a bias towards the majority class, leading to inaccurate predictions for the minority class. If you find that your dataset is imbalanced, you can apply techniques like oversampling, undersampling, or SMOTE to make it balanced.\n",
        "\n",
        "* Analyzing image dimensions: Understanding the dimensions of the images in your dataset is important because it can impact how you preprocess your images. If your images are of different sizes, you'll need to resize them so they're all the same size before you can feed them into your machine learning model. Also, very large images may need to be downscaled to prevent out-of-memory errors and to speed up training.\n",
        "\n",
        "* Color distribution: Understanding the color distribution in your images can help you decide whether to use color or grayscale images in your model. If color plays an important role in distinguishing between cats and dogs, then you might want to use color images in your model. On the other hand, if color isn't important, then you might want to convert your images to grayscale, which can simplify your model and speed up training.\n",
        "\n",
        "* Visualizing samples: Displaying a sample image from each class can help you understand what your raw data looks like and whether any preprocessing steps (like resizing, cropping, or normalization) might be needed.\n",
        "\n",
        "* Identifying potential issues: EDA can help you identify potential issues in your dataset, such as missing data, duplicate data, or incorrect labels, that could negatively impact your model's performance.\n"
      ],
      "metadata": {
        "id": "u7KvisaaSXS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Get class names\n",
        "class_names = os.listdir(train_dir)\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "# Count number of images per class\n",
        "class_counts = {}\n",
        "for class_name in class_names:\n",
        "    class_counts[class_name] = len(os.listdir(os.path.join(train_dir, class_name)))\n",
        "\n",
        "# Display the number of images per class\n",
        "print(f\"Number of images per class: {min(class_counts.values())} - {max(class_counts.values())}\")\n",
        "\n",
        "# Display some samples from the dataset\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(len(class_names)):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    img_class = class_names[i]\n",
        "    img_dir = os.path.join(train_dir, img_class)\n",
        "    img_name = os.listdir(img_dir)[0]\n",
        "    img = Image.open(os.path.join(img_dir, img_name))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Class: {img_class}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OV3NK2PzSZs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check for imbalance in the dataset\n",
        "\n",
        "Imbalance in the dataset can lead to poor model performance, as the model might overfit to the majority class. We've already calculated the number of images per class, but we can also visualize this information:"
      ],
      "metadata": {
        "id": "CA9b5OyoXAVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(class_counts.keys(), class_counts.values())\n",
        "plt.title('Number of images per class')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NfT-AbgtXBx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze image dimensions\n",
        "\n",
        "Some datasets may have images of varying dimensions. Understanding these dimensions can be crucial when designing the input layer of your model:"
      ],
      "metadata": {
        "id": "XsxC5_ZiXGUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "image_dims = []\n",
        "\n",
        "for class_name in class_names:\n",
        "    img_dir = os.path.join(train_dir, class_name)\n",
        "    for img_name in os.listdir(img_dir):\n",
        "        img = Image.open(os.path.join(img_dir, img_name))\n",
        "        image_dims.append(img.size)\n",
        "\n",
        "# Convert to a numpy array and calculate stats\n",
        "image_dims = np.array(image_dims)\n",
        "print(f'Image dimensions: min={np.min(image_dims, axis=0)}, max={np.max(image_dims, axis=0)}, avg={np.mean(image_dims, axis=0)}')"
      ],
      "metadata": {
        "id": "tDlThyJYXHUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing color distribution\n",
        "\n",
        "Understanding the color distribution of your images can provide insights into whether color-based features might be important:"
      ],
      "metadata": {
        "id": "TjIejRGiXU-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "color_data = []\n",
        "\n",
        "# Use a sample of images, otherwise it may take a long time\n",
        "sample_size = 100\n",
        "\n",
        "for class_name in class_names:\n",
        "    img_dir = os.path.join(train_dir, class_name)\n",
        "    img_names = os.listdir(img_dir)\n",
        "    np.random.shuffle(img_names)\n",
        "    for img_name in img_names[:sample_size]:\n",
        "        img = Image.open(os.path.join(img_dir, img_name))\n",
        "        color_data.extend(np.array(img).reshape(-1, 3))\n",
        "\n",
        "# Convert to a numpy array\n",
        "color_data = np.array(color_data)\n",
        "\n",
        "# Calculate color stats\n",
        "print(f\"Color data: min={np.min(color_data, axis=0)}, max={np.max(color_data, axis=0)}, avg={np.mean(color_data, axis=0)}\")"
      ],
      "metadata": {
        "id": "wuu6gjXEXV_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "The image_dataset_from_directory function already takes care of resizing the images. However, you might want to normalize the pixel values to the range [0, 1]:"
      ],
      "metadata": {
        "id": "QrNf-NaSYVMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "train_ds = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_dataset.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "metadata": {
        "id": "_5qAUPfdYWf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building\n",
        "\n"
      ],
      "metadata": {
        "id": "HuKDG4RZYp5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "lIIQpdgWZTTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a data generator to handle data preprocessing and data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,        # Normalize pixel values to [0,1]\n",
        "    shear_range=0.2,       # Randomly applies shearing transformation\n",
        "    zoom_range=0.2,        # Randomly zooms in the image\n",
        "    horizontal_flip=True)  # Randomly flips the image horizontally\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255) # For validation/test data, only rescaling is needed\n",
        "\n",
        "# Use the data generator to load data from directories\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(64, 64),  # All images will be resized to 64x64\n",
        "    batch_size=32,\n",
        "    class_mode='binary')   # Since we use binary_crossentropy loss, we need binary labels\n",
        "\n",
        "val_set = test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')"
      ],
      "metadata": {
        "id": "QXpa-BjxZT-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network (CNN) with multiple convolution and pooling layers, dropout for regularization, and a larger fully connected layer before the final output layer.\n",
        "\n",
        "This model uses two sets of Conv2D, MaxPooling2D, and Dropout layers, followed by a larger Dense layer. The Dropout layers are used for regularization and help to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training."
      ],
      "metadata": {
        "id": "HovlFL3pZvIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# First Convolution Layer\n",
        "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(64, 64, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))  # Dropout regularization\n",
        "\n",
        "# Second Convolution Layer\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))  # Dropout regularization\n",
        "\n",
        "# Flattening Layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully Connected Layer\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Dropout regularization\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "CeJjN53YZq-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "pprhwCjkaDFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "W0U-n5gwEoUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualize the architecture of your model using the plot_model function from keras.utils"
      ],
      "metadata": {
        "id": "pQB6jR9oadiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Plot the model\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Display the image\n",
        "img = mpimg.imread('model.png')\n",
        "plt.figure(figsize=(10,18))\n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5XUxBrM7aeOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model and save the history\n",
        "history= model.fit(train_set\n",
        "          , steps_per_epoch=800\n",
        "          , epochs=30\n",
        "          , validation_data=val_set\n",
        "          , validation_steps=500)"
      ],
      "metadata": {
        "id": "MrcX890qaFJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "GLKIAuGBdasp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Training and Validation Accuracy and Loss Over Epochs\n",
        "\n",
        "You can plot the training and validation accuracy and loss over epochs to visualize how your model is learning"
      ],
      "metadata": {
        "id": "lY7mYbQBdc74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(12, 12))\n",
        "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
        "ax[0].plot(history.history['val_loss'], color='r', label=\"Validation loss\",axes =ax[0])\n",
        "legend = ax[0].legend(loc='best', shadow=True)\n",
        "\n",
        "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
        "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "legend = ax[1].legend(loc='best', shadow=True)"
      ],
      "metadata": {
        "id": "S-uCizdRdiTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table layout that allows visualization of the performance of an algorithm. It's extremely useful for measuring Recall, Precision, F1-Score and AUC-ROC Curve."
      ],
      "metadata": {
        "id": "VDNDnxybd0KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Get the model predictions\n",
        "predictions = model.predict(val_set)\n",
        "y_pred = [1 * (x[0]>=0.5) for x in predictions]\n",
        "\n",
        "# Get the actual labels\n",
        "y_true = val_set.classes\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "plt.title('Confusion matrix')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6aT2D0Pqd2Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report\n",
        "\n",
        "A Classification report is used to measure the quality of predictions from a classification algorithm. It shows precision, recall, F1-Score, and support for each class."
      ],
      "metadata": {
        "id": "-R1aQMthd5Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "SZpMqGfed6hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " visualize some predictions on the validation set. We'll plot the images along with their true labels and predicted labels."
      ],
      "metadata": {
        "id": "uy4FeILMeIiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of data from the validation set\n",
        "x_val_sample, y_val_sample = next(iter(val_set))\n",
        "\n",
        "# Make predictions on the sample\n",
        "y_pred_sample = model.predict(x_val_sample)\n",
        "y_pred_sample_classes = [1 * (x[0]>=0.5) for x in y_pred_sample]\n",
        "\n",
        "# Define class labels\n",
        "class_labels = ['Cat', 'Dog']\n",
        "\n",
        "# Plot the images, true labels, and predicted labels\n",
        "fig, axes = plt.subplots(5, 5, figsize=(15,15))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in np.arange(0, 25):\n",
        "    axes[i].imshow(x_val_sample[i])\n",
        "    axes[i].set_title(\"True: %s \\nPredict: %s\" % (class_labels[int(y_val_sample[i])], class_labels[y_pred_sample_classes[i]]))\n",
        "    axes[i].axis('off')\n",
        "    plt.subplots_adjust(wspace=1)"
      ],
      "metadata": {
        "id": "I6K44KFseJGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script gets a batch of images and labels from the validation set, makes predictions on these images, and then plots the images along with the true and predicted labels."
      ],
      "metadata": {
        "id": "z0hu48iSePFU"
      }
    }
  ]
}